\section{What is Dimensionality Reduction?}
    Dimensionality reduction is a technique used to reduce the number of features in a dataset while retaining as much of the important information as possible. In other words, it is a process of transforming high-dimensional data into a lower-dimensional space that still preserves the essence of the original data.

    In machine learning, high-dimensional data refers to data with a large number of features or variables. The curse of dimensionality is a common problem in machine learning, where the performance of the model deteriorates as the number of features increases. This is because the complexity of the model increases with the number of features, and it becomes more difficult to find a good solution. In addition, high-dimensional data can also lead to overfitting, where the model fits the training data too closely and does not generalize well to new data.

    Dimensionality reduction can help to mitigate these problems by reducing the complexity of the model and improving its generalization performance. There are two main approaches to dimensionality reduction: feature selection and feature extraction.

    \subsection{Feature Selection}
        Feature selection involves selecting a subset of the original features that are most relevant to the problem at hand. The goal is to reduce the dimensionality of the dataset while retaining the most important features. There are several methods for feature selection, including filter methods, wrapper methods, and embedded methods. 
        \begin{itemize}
            \item Filter methods rank the features based on their relevance to the target variable.
            \item Wrapper methods use the model performance as the criteria for selecting features. 
            \item Embedded methods combine feature selection with the model training process.
        \end{itemize}
    \subsection{Feature Extraction}
        Feature extraction involves creating new features by combining or transforming the original features. The goal is to create a set of features that captures the essence of the original data in a lower-dimensional space. There are several methods for feature extraction, including principal component analysis (PCA), linear discriminant analysis (LDA), and t{-}distributed stochastic neighbor embedding (t{-}SNE). PCA is a popular technique that projects the original features onto a lower-dimensional space while preserving as much of the variance as possible.
        \subsubsection{Principal Component Analysis}
            PCA is an unsupervised method of dimensionality reduction that aims to find the directions of maximum variance in a dataset. The idea is to find a smaller set of variables or features that capture the most important patterns in the data. PCA is often used to preprocess data for machine learning algorithms.
            PCA works by first centering the data around its mean and then finding the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the directions of maximum variance, while the eigenvalues represent the amount of variance explained by each eigenvector. The eigenvectors are then used to project the data onto a lower-dimensional space.
            The number of principal components to keep is determined by the amount of variance we want to retain. Typically, we keep the principal components that explain a certain percentage of the total variance in the data. After a certain number, the variance explained by the PCA components, cumulatively, does not increase much with the increasing components.
        \subsubsection{Linear Discriminant Analysis}
            LDA is a supervised method of dimensionality reduction that aims to find the linear combination of features that best separates the classes in a dataset. The idea is to reduce the dimensionality of the data while preserving the information that is most relevant for class discrimination.
            LDA works by first calculating the mean and covariance matrix for each class in the data. It then calculates the between-class scatter matrix and the within-class scatter matrix. The goal is to find a projection that maximizes the ratio of the between-class scatter matrix to the within-class scatter matrix. This projection is the linear discriminant function.
            Once the linear discriminant function is found, we can project the data onto this function to obtain the reduced representation of the data. The resulting transformed data can be used for classification.
        \subsubsection{PCA vs LDA â€” When to use what?}
            PCA and LDA are both powerful techniques for dimensionality reduction, but they have different objectives and assumptions.
            One of the main differences is in their objectives. PCA aims to find the directions of maximum variance in the data, while LDA aims to find the projection that best separates the classes in the data.
            Another difference is in their assumptions. PCA is an unsupervised method that does not take into account the class labels in the data. LDA, on the other hand, is a supervised method that assumes that the data is normally distributed and that the covariance matrices for each class are equal.
            Finally, PCA is often used for exploratory data analysis and preprocessing of data for machine learning algorithms, while LDA is often used for classification and feature selection.
