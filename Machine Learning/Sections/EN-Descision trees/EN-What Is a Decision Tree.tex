\section{What is a Descision Tree?}
    A decision tree is a flowchart-like tree structure where each internal node denotes the feature, branches denote the rules and the leaf nodes denote the result of the algorithm. 
    It is a versatile supervised machine-learning algorithm, which is used for both classification and regression problems. 

\section{What Is the Structure of a Decision Tree?}
    A tree consists of 2 major components:
    \begin{itemize}
        \item Decision node: the point where you make a decision
        \item Leaf node: the output of said decision; it does not contain any further branches
    \end{itemize}
    The algorithm starts from the first decision node, known as the root node. It represents the entire dataset, which is further divided into 2 or more homogeneous sets. The decision nodes represent the dataset's features, branches denote the decision rules, and each leaf node signifies the outcome.    

\section{Decision Tree Terminologies}
    \begin{enumerate}
        \item Root Node: It is the topmost node in the tree,  which represents the complete dataset. It is the starting point of the decision-making process. The feature with the highest inforation gain is choses as a root node.
        \item Decision/Internal Node: A node that symbolizes a choice regarding an input feature. Branching off of internal nodes connects them to leaf nodes or other internal nodes.
        \item Leaf/Terminal Node: A node without any child nodes that indicates a class label or a numerical value.
        \item Pure Node: Pure nodes are those that have one class â€” hence the term pure.
        \item Splitting: The process of splitting a node into two or more sub-nodes using a split criterion and a selected feature.
        \item Branch/Sub-Tree: A subsection of the decision tree starts at an internal node and ends at the leaf nodes.
        \item Parent Node: The node that divides into one or more child nodes.
        \item Child Node: The nodes that emerge when a parent node is split.
        \item Impurity: A measurement of the target variable's homogeneity in a subset of data. It refers to the degree of randomness or uncertainty in a set of examples. The Gini index and entropy are two commonly used impurity measurements in decision trees for classifications task 
        \item Variance: Variance measures how much the predicted and the target variables vary in different samples of a dataset. It is used for regression problems in decision trees. Mean squared error, Mean Absolute Error, friedman mse, or Half Poisson deviance are used to measure the variance for the regression tasks in the decision tree.        
        \item Information Gain: Information gain is a measure of the reduction in impurity achieved by splitting a dataset on a particular feature in a decision tree. The splitting criterion is determined by the feature that offers the greatest information gain, It is used to determine the most informative feature to split on at each node of the tree, with the goal of creating pure subsets
        \item Pruning: The process of removing branches from the tree that do not provide any additional information or lead to overfitting.
    \end{enumerate}