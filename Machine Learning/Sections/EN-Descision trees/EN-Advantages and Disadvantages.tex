\section{Advantages}
    \begin{itemize}
        \item Decision trees are easy to understand. Because of their structure, which follows the natural flow of human thought, most people will have little trouble interpreting them. In addition, visualizing the model is effortless and allows you to see exactly what decisions are being made.
        \item There is little to no need for data preprocessing. Unlike other algorithms, decision trees take less time to model as they require less coding, analysis, or even dummy variables. The reason is that the technique looks at each data point individually instead of the set as a whole.
        \item Versatile when it comes to data. In other words, standardizing the collected data is not a necessity. You can imbue both numerical and categorical data into the model as it’s able to work with features of both types.
    \end{itemize}

\section{Disadvantages}
    \begin{itemize}
        \item There is a tendency to overfit. Essentially, the model performs so well on the training data that it compromises the decision-making process. You can prevent this by either stopping the decision tree before it has a chance to do so or, alternatively, letting it grow and then pruning the decision tree after overfitting occurs.
        \item Mathematical equations are more costly. Not only does the decision tree require more time to calculate, but it also consumes more memory. This is not ideal as sometimes you will have to work with substantial amounts of data and stricter deadlines , efficiency is of the essence.
        \item Decision trees can be unstable. For example, a minor modification of the data can lead to significant changes – perhaps even generating a new tree with contrary results. Another instance is the model producing biased decisions if some of the classes dominate over the rest.
    \end{itemize}