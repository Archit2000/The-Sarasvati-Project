\section{Hyperparameters}
    \begin{enumerate}
        \item Max Depth: The name of hyperparameter max depth is suggested the maximum depth that we allow the tree to grow to. The deeper you allow, the more complex our model will become; in the case of training error, it is easy to see what will happen. If we increase the max depth value, training error will always go down but let us consider a case for testing error; if we set the max depth too high, then without capturing the useful patterns, the decision tree might simply overfit the training data, this will cause the testing error to increase, but if we set the value of it to too low that is also not good then the decision tree might be having little flexibility to capture the patterns and interaction in the training data. This will also cause the testing error to increase, which is a case of underfitting so we have to find the right max depth using hyperparameter tuning either grid search or at the best possible values of max depth the random search might arrive.
        \item Min Samples Split: In a decision tree, there are multiple nodes, some of which are internal nodes, while others are called leaf nodes. Internal nodes can further split into child nodes. The min samples split hyperparameter specifies the minimal number of samples required to divide a node. This hyperparameter can accept various values: you can specify an integer to denote the minimum number of samples required in an internal node, or you can use a fraction to denote the minimum percentage of samples required in an internal node.        
        \item Min Samples Leaf: Let us see another parameter which is called min samples leaf, as we have already seen a leaf node is a node without any children, so we cannot split a leaf node any further, so min samples leaf is the minimum number of samples that we can specify to term a given node as a leaf node so that we do not want to split it further. For example, we start off with 10,000 samples, and we reach a node wherein we just have 100 samples; there is no point in splitting further as you would tend to overfit the training data that you have, so by using this hyperparameter smartly, we can also avoid overfitting, and this parameter is similar to min samples splits. However, this describes the minimum number of samples at the leaf that is a base of the tree.
        \item Min Weight Fraction Leaf: This is also another type of decision tree hyperparameter, which is called min weight fraction, it is the fraction of the input samples that are required at the leaf node where sample weight determined weight, in this way, we can deal with class unbalancing, and the class unbalancing can be done by sampling an equal number of samples from each class. Also, when we biased it towards dominant classes then that are not aware of the sample weights like min sample leaf, by using weight-based criteria, it is easier to optimize the structure of the tree if the samples are weighted, min weight fraction leaf, in which the leaf nodes contain at least a fraction of the overall sum of the weights.
        \item Class Weight: This is also a hyperparameter called class weight in which weight associated with classes or it is used to provide weight for each output class; this actually means when the algorithm calculates impurity to make the split at each node, then the resulting child node are weighted by class weight by giving the child sample weight, distribution of our classes has been start then the weight of class and then depending on where our tree lean, we can try to increase the weight of the other class so that algorithm penalizes the sample of one class relative to the other.
    \end{enumerate}